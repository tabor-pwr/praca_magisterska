\chapter{Charakterystyka algorytmów uczenia maszynowego}
\section{Uczenie nadzorowane (Supervised Learning)}
Uczenie nadzorowane to technika, w której algorytm otrzymuje gotowy zestaw danych
z wcześniej określonymi etykietami i uczy się na ich podstawie predyktować etykiety dla nowych nieznanych danych.
Dane uczące zawierają zmienne wejściowe oraz zmienną predykowaną (etykietę).
Standardowy proces uczenia nadzorowanego obejmuje podział zbioru danych na zbiór
treningowy, walidacyjny oraz testowy. Trening polega na dopasowaniu parametrów modelu
do danych treningowych poprzez minimalizację funkcji straty, która mierzy różnicę
między przewidywaniami modelu a rzeczywistymi etykietami. Po zakończeniu treningu danych
model zostaje poddany ocenie na zbiorze walidacyjnym w celu doboru hiperparametrów oraz monitorowaniu uczenia maszynowego,
aby zapobiec przeuczeniu modelu. Ostateczna ocena odbywa się na zbiorze testowym na danych nieznanych dla modelu i na podstawie tego modelu określana
wartość dokładności, precyzji, recall, F1 (dla klasyfikacji) lub MSE, MAE (dla regresji)\citep{bishop2006,hastie2009}.
\subsection{Regresja liniowa (Linear Regression)}
Regresja liniowa to podstawowa technika statystyczna i uczenia maszynowego stosowana do modelowania zależności między zmienną zależną (predykowaną) a jedną lub więcej zmiennymi niezależnymi (cechami). Celem regresji liniowej jest znalezienie liniowej funkcji, która najlepiej opisuje związek między zmiennymi, umożliwiając przewidywanie wartości zmiennej zależnej na podstawie wartości zmiennych niezależnych.

\textbf{Regresja liniowa prosta.}
W przypadku pojedynczej zmiennej niezależnej model można opisać równaniem:
\begin{equation}
y = \alpha + \beta x + \varepsilon,
\end{equation}
gdzie \(y\) to zmienna zależna, \(x\) to zmienna niezależna, \(\alpha\) to wyraz wolny, \(\beta\) to współczynnik nachylenia linii regresji, a \(\varepsilon\) to składnik losowy (błąd modelu).

\textbf{Regresja wieloraka.}
Regresja wieloraka jest rozszerzeniem regresji prostej, poprzez używanie wielu zmiennych niezależnych. Zakłada się liniową zależność między zmienną zależną a kombinacją liniową zmiennych niezależnych:
\begin{equation}
y = \alpha + \sum_{i=1}^{p} \beta_i x_i + \varepsilon,
\end{equation}
gdzie \(y\) to zmienna zależna, \(\alpha\) to wyraz wolny, \(x_i\) to zmienne niezależne, \(\beta_i\) to współczynniki regresji określające wpływ danej zmiennej na zmienną zależną, a \(\varepsilon\) to składnik losowy (błąd modelu).

Celem algorytmu regresji (prostej i wielorakiej) jest znalezienie optymalnych wartości współczynników \(\beta_i\), które minimalizują błąd predykcji. Współczynniki można dobrać za pomocą różnych metod, jednak najczęściej stosuje się metodę najmniejszych kwadratów (OLS — Ordinary Least Squares), która minimalizuje sumę kwadratów różnic między rzeczywistymi a przewidywanymi wartościami zmiennej zależnej.

\textbf{Estymator OLS (macierzowy zapis).}
\begin{equation}\label{eq:ols}
\widehat{\boldsymbol{\beta}} = (X^\top X)^{-1} X^\top \mathbf{y}
\end{equation}

Wzór \eqref{eq:ols} to macierzowy zapis estymatora OLS. Wyjaśnienie:
\begin{itemize}
  \item \(X\) — macierz projektująca (design matrix) o wymiarach \(n\times p\) (lub \(n\times (p+1)\), jeśli dodano kolumnę jedynek dla wyrazu wolnego),  
  \item \(\mathbf{y}\) — wektor obserwacji o wymiarze \(n\times 1\),  
  \item \(\widehat{\boldsymbol{\beta}}\) — wektor estymowanych współczynników o wymiarze \(p\times 1\).
\end{itemize}

Aby wyrażenie było poprawne, macierz \(X^\top X\) musi być odwracalna (brak doskonałej multikolinearności). Przy klasycznych założeniach (m.in. \(E[\varepsilon]=0\), \(\text{Var}(\varepsilon)=\sigma^2 I\)) estymator jest nieobciążony, a jego wariancja wynosi \(\text{Var}(\widehat{\boldsymbol{\beta}})=\sigma^2 (X^\top X)^{-1}\). Gdy \(X^\top X\) jest źle uwarunkowana lub nieodwracalna, stosuje się regularyzację (np. Ridge) lub metody numeryczne (gradient descent, SVD)\citep{james2013}.

\vspace{0.5cm}

\textbf{Inne metody estymacji współczynników regresji liniowej:}
\begin{itemize}
    \item Metoda gradientu prostego (Gradient Descent) — iteracyjna metoda optymalizacji minimalizująca funkcję straty poprzez aktualizację współczynników w kierunku przeciwnym do gradientu \citep{bishop2006,murphy2012}.
    \item Metoda najmniejszych modułów (Least Absolute Deviations) — minimalizuje sumę bezwzględnych różnic między rzeczywistymi a przewidywanymi wartościami, co czyni ją bardziej odporną na wartości odstające \citep{birkes1993}. 
    \item Metoda Ridge Regression — wprowadza regularyzację L2, karę za duże wartości współczynników, co pomaga w radzeniu sobie z problemem multikolinearności i przeuczenia modelu \citep{hoerl1970,hastie2009}.
    \item Metoda Lasso Regression — regularyzacja L1, która może prowadzić do zerowania niektórych współczynników, skutkując modelem o mniejszej liczbie cech (automatyczny wybór cech) \citep{tibshirani1996}.
    \item Metoda Elastic Net — łączy regularyzację L1 i L2, co pozwala na lepsze dostosowanie modelu do danych \citep{zou2005}.
    \item Metoda SVD (Singular Value Decomposition) — rozkłada macierz projektującą na składniki, umożliwiając stabilne obliczenie współczynników regresji nawet w przypadku kolinearności cech \citep{golub1996,press2007}.
    \item Metoda Bayesian Regression — wykorzystuje podejście bayesowskie do estymacji współczynników, uwzględniając niepewność i priorytety w modelu \citep{gelman2006,bishop2006}.
    \item QR Decomposition — rozkłada macierz projektującą na iloczyn macierzy ortogonalnej i górnotrójkątnej, co umożliwia efektywne rozwiązanie układu równań regresji \citep{golub1996,press2007}.
\end{itemize}

\subsection{Regresja logistyczna (Logistic Regression)}
Regresja logistyczna to technika statystyczna i uczenia maszynowego stosowana do modelowania zależności między zmienną zależną a jedną lub więcej zmiennymi niezależnymi, gdy zmienna zależna przyjmuję wartości binarne (np. 0 lub 1, tak lub nie). Celem regresji logistycznej jest przewidywanie prawdopodobieństwa przynależności do jednej z dwóch klas na podstawie wartości zmiennych niezależnych.

\textbf{Model regresji logistycznej.}
Model regresji logistycznej można zapisać jako:
\begin{equation}
P(Y=1|X) = \sigma(\boldsymbol{\beta}^\top X) = \frac{1}{1 + e^{-\boldsymbol{\beta}^\top X}}
\end{equation}
gdzie:
\begin{itemize}
  \item \(P(Y=1|X)\) — prawdopodobieństwo, że zmienna zależna \(Y\) przyjmuje wartość 1, biorąc pod uwagę zmienne niezależne \(X\),
  \item \(\sigma(z)\) — funkcja sigmoidalna, która przekształca dowolną wartość rzeczywistą \(z\) w przedział (0, 1).
\end{itemize}

\textbf{Estymacja parametrów.}
Parametry modelu \(\boldsymbol{\beta}\) są estymowane za pomocą metody największej wiarygodności (Maximum Likelihood Estimation, MLE). Celem jest maksymalizacja funkcji wiarygodności:
\begin{equation}
L(\boldsymbol{\beta}) = \prod_{i=1}^{n} P(Y_i|X_i; \boldsymbol{\beta})
\end{equation}
co jest równoważne minimalizacji funkcji straty:
\begin{equation}
J(\boldsymbol{\beta}) = -\sum_{i=1}^{n} \left[ Y_i \log(P(Y_i|X_i; \boldsymbol{\beta})) + (1 - Y_i) \log(1 - P(Y_i|X_i; \boldsymbol{\beta})) \right]
\end{equation}

\textbf{Właściwości modelu.}
Model regresji logistycznej ma kilka istotnych właściwości:
\begin{itemize}
  \item Wynikiem modelu jest prawdopodobieństwo, co czyni go odpowiednim narzędziem do klasyfikacji binarnej.
  \item Model jest odporny na wartości odstające, ponieważ wykorzystuje funkcję sigmoidalną.
  \item Można go łatwo rozszerzyć na problemy wieloklasowe (np. regresja wielomianowa).
\end{itemize}

\subsection{\(k\)-Najbliższych Sąsiadów (k-Nearest Neighbors)}
Algorytm \(k\)-Najbliższych Sąsiadów umieszcza dane wejściowe w przestrzeni wielowymiarowej i klasyfikuje je na podstawie etykiet najbliższych sąsiadów w tej przestrzeni.
Przestrzeń jest definiowana przez cechy danych, zbiór danych posiadający \(x\) cech jest reprezentowany w \(x\)-wymiarowej przestrzeni.
Algorytm klasyfikując dany obiekt oblicza odległości między nim a wszystkimi innymi obiektami w przestrzeni, a następnie wybiera \(k\) najblizszych sąsiadów.
Wartość \(k\) jest ustalana ustalana przed rozpoczęciem działania algorytmu. Niska wartość parametru \(k\) jest bardziej podatna na szumy w danych, podczas gdy wysoka wartość \(k\) może prowadzić do nadmiernego uogólnienia modelu \citep{cover1967}.

\vspace{0.5cm}
Algorytm \(k\)-najbliższych sąsiadów może wykorzystywać różne metryki do obliczania odległości m.in:

\vspace{0.5cm}
\textbf{Metryka Euklidesowa:}
Najpowszechniejsza metryka używana do obliczania odległości między dwoma punktami w przestrzeni wielowymiarowej. Definiowana jest jako:
\begin{equation}
d(p, q) = \sqrt{\sum_{i=1}^{n} (p_i - q_i)^2}
\end{equation}
gdzie \(p\) i \(q\) to dwa punkty w przestrzeni, a \(n\) to liczba wymiarów. Wyliczanie odległości metryką euklidesową polega na policzeniu różnicy ogległości w każdym wymiarze dla dwóch punktów,
zsumowaniu kwadratów tych różnic, a następnie wyciągnięciu pierwiastka kwadratowego z tej sumy \citep{duda2001,bishop2006}.

\vspace{0.5cm}
\textbf{Metryka Manhattan:}
Metryka Manhattan, nazywana również metryką taksówkową lub L1, mierzy odległość miedzy dwoma punktami jako sumę watości bezwzględnych z różnic współżędnych.
Definiowana jest jako:
\begin{equation}
d(p, q) = \sum_{i=1}^{n} |p_i - q_i|
\end{equation}
gdzie \(p\) i \(q\) to dwa punkty w przestrzeni, a \(n\) to liczba wymiarów. Obliczana jest jest różnica wartości \(p\) i \(q\) dla każdego wymiaru, a następnie sumowane są wartości bezwzględne tych różnic \citep{duda2001}.

\vspace{0.5cm}
\textbf{Metryka Kosinusowa:}
Metryka Kosinusowa mierzy wyznacza odległość między dwoma punktami na podstawie wyliczonego kąta między nimi.
Definiowana jest jako:
\begin{equation}
d(p, q) = 1 - \frac{p \cdot q}{\|p\| \|q\|}
\end{equation}
gdzie \(p\) i \(q\) to dwa punkty w przestrzeni, \(p \cdot q\) to iloczyn skalarny wektorów \(p\) i \(q\), a \(\|p\|\) i \(\|q\|\) to normy (długości) tych wektorów.
Normy długości wektorów są obliczane jako pierwiastki kwadratowe z sumy kwadratów ich współrzędnych \citep{manning2008,bishop2006}.

\vspace{0.5cm}
\textbf{Metryka Minkowskiego:}
Metryka Minkowskiego jest uogólnieniem metryk Euklidesowej i Manhattan. Umożliwa regulowanie sposobu obliczania odległości poprzez parametr \(p\).
Definiowana jest jako:
\begin{equation}
d(p, q) = \left( \sum_{i=1}^{n} |
p_i - q_i|^p \right)^{\frac{1}{p}}
\end{equation}
gdzie \(p\) i \(q\) to dwa punkty w przestrzeni, \(n\) to liczba wymiarów, a \(p\) to parametr regulujący sposób obliczania odległości.
Dla \(p=1\) metryka Minkowskiego jest równoważna metryce Manhattan, a dla \(p=2\) jest równoważna metryce Euklidesowej \citep{hastie2009}.

\subsection{Drzewa decyzyjne (Decision Trees)}
Drzewa decyzyjne to algorytm uczenia maszynowego, która służy do podejmowania decyzji na podstawie zestawu reguł, które są reprezentowane w formie drzewa.
Drzewo decyzyjne składa się z korzenia, które jest cechą dzielącą dane na grupy, węzłów wewnętrznych, które reprezentują pytania dotyczące cech danych, oraz liści, które reprezentują ostateczne decyzje lub klasyfikacje.
Algorytm budowy drzewa decyzyjnego polega na iteracyjnym dzieleniu danych na podzbiory na podstawie cech, które najlepiej rozdzielają dane według określonego kryterium.
Drzewo decyzyjne jest budowane, aż wszystkie elementy w podzbiorze należą do tej samej klasy lub nie ma już cech do podziału, osiągnie maksymalną głębokość lub kiedy dalszy podział nie poprawia jakości klasyfikacji \citep{quinlan1986,breiman1984}.

\vspace{0.5cm}
Drzewo decyzyjne do wyboru najlepszego podziału danych może wykorzystywać różne kryteria, m.in:

\vspace{0.5cm}
\textbf{Entropia:}
Entropia jest miarą niepewności lub nieuporządkowania w zbiorze danych. Entropia jest wykorzystywana do oceny jakości podziału danych na podstaiwe cechy.
Definiowana jest jako:
\begin{equation} 
H(S) = - \sum_{i=1}^{c} p_i \log_2(p_i)
\end{equation}
gdzie \(S\) to zbiór danych, \(c\) to liczba klas, a \(p_i\) to proporcja elementów należących do klasy \(i\) w zbiorze \(S\).
Entropia obliczana jest jako suma iloczynów proporcji klas i logarytmów tych proporcji, a następnie mnożona przez -1 \citep{shannon1948,quinlan1986}.

\vspace{0.5cm}
\textbf{Wskaźnik Gini (Gini Impurity):}
Wskaźnik Gini mierzy prawdopodobieństwo błędnej klasyfikacji losowo wybranego elementu, gdyby został on oznaczony losowo według rozkładu etykiet w węźle. Im niższy wskaźnik Gini, tym bardziej jednorodny jest węzeł. Wskaźnik Gini dla zbioru \(S\) jest definiowany jako:
\begin{equation}
\text{Gini}(S) = 1 - \sum_{i=1}^{c} p_i^2
\end{equation}
gdzie \(c\) to liczba klas, a \(p_i\) to proporcja przykładów w klasie \(i\). Wskaźnik Gini jest używany w algorytmie CART (Classification and Regression Trees) ze względu na swoją prostotę obliczeniową i dobrą wydajność \citep{breiman1984}.

\vspace{0.5cm}
\textbf{Zysk informacji (Information Gain):}
Zysk informacji mierzy redukcję entropii osiągniętą przez podział zbioru danych według danego atrybutu. Jest to różnica między entropią zbioru nadrzędnego a ważoną sumą entropii zbiorów potomnych. Zysk informacji dla atrybutu \(A\) w zbiorze \(S\) definiuje się jako:
\begin{equation}
\text{IG}(S, A) = H(S) - \sum_{v \in \text{Values}(A)} \frac{|S_v|}{|S|} H(S_v)
\end{equation}
gdzie \(\text{Values}(A)\) to zbiór wszystkich możliwych wartości atrybutu \(A\), \(S_v\) to podzbiór \(S\), dla którego atrybut \(A\) ma wartość \(v\), a \(H(S)\) to entropia zbioru. Algorytm ID3 wybiera atrybut o największym zysku informacji \citep{quinlan1986}.

\vspace{0.5cm}
\textbf{Współczynnik zysku (Gain Ratio)}
Współczynnik zysku jest modyfikacją zysku informacji, która koryguje tendencję do faworyzowania atrybutów o wielu wartościach. Normalizuje zysk informacji przez podzielenie go przez tzw. split information, która mierzy szerokość i jednolitość podziału. Współczynnik zysku dla atrybutu \(A\) w zbiorze \(S\) definiuje się jako:
\begin{equation}
\text{GainRatio}(S, A) = \frac{\text{IG}(S, A)}{\text{SplitInfo}(S, A)}
\end{equation}
gdzie \(\text{IG}(S, A)\) to zysk informacji dla atrybutu \(A\), a \(\text{SplitInfo}(S, A)\) to informacja o podziale, definiowana jako:
\begin{equation}
\text{SplitInfo}(S, A) = -\sum_{v \in \text{Values}(A)} \frac{|S_v|}{|S|} \log_2\left(\frac{|S_v|}{|S|}\right)
\end{equation}
gdzie \(\text{Values}(A)\) to zbiór wszystkich możliwych wartości atrybutu \(A\), \(S_v\) to podzbiór \(S\) zawierający elementy, dla których atrybut \(A\) ma wartość \(v\), \(|S_v|\) to liczba elementów w podzbiorze \(S_v\), a \(|S|\) to całkowita liczba elementów w zbiorze \(S\). Informacja o podziale mierzy, jak bardzo atrybut dzieli dane. Im bardziej równomierny podział, tym wyższa wartość \(\text{SplitInfo}\), co zmniejsza współczynnik zysku i zapobiega faworyzowaniu atrybutów o wielu unikalnych wartościach. Współczynnik zysku jest używany w algorytmie C4.5 jako ulepszona wersja ID3 \citep{quinlan1993}.

\vspace{0.5cm}
\textbf{Redukcja wariancji (Variance Reduction)}
Redukcja wariancji jest kryterium stosowanym w drzewach regresyjnych, gdzie celem jest przewidywanie wartości ciągłych, a nie kategorii. Kryterium to wybiera podział, który maksymalnie redukuje wariancję wartości docelowych w węzłach potomnych. Redukcja wariancji dla podziału zbioru \(S\) na podzbiory \(S_{\text{left}}\) i \(S_{\text{right}}\) definiuje się jako:
\begin{equation}
\text{VarReduction}(S) = \text{Var}(S) - \left(\frac{|S_{\text{left}}|}{|S|} \text{Var}(S_{\text{left}}) + \frac{|S_{\text{right}}|}{|S|} \text{Var}(S_{\text{right}})\right)
\end{equation}
gdzie \(\text{Var}(S)\) oznacza wariancję wartości docelowych w zbiorze \(S\). Algorytm CART dla regresji wykorzystuje to kryterium do budowy drzew regresyjnych \citep{breiman1984}.

\subsection{Las losowy (Random Forest)}
Las losowy to algorytm uczenia maszynowego, który łączy wiele drzew decyzyjnych w celu poprawy dokładności przewidywań i redukcji przeuczenia.
Algorytm ten działa na zasadzie tworzenia wielu niezależnych drzew decyzyjnych, z których każde jest trenowane na losowym podzbiorze danych i losowym podzbiorze cech.
Ostateczna predykcja lasu losowego jest uzyskiwana przez agregację wyników wszystkich drzew. 
Dla klasyfikacji stosuje się głosowanie większościowe, a dla regresji średnią arytmetyczną \citep{breiman2001}.

\vspace{0.5cm}
Las losowy uczy się poprzez losowanie i budowanie wielu drzew decyzyjnych.
Każde drzewo dostaje losowy podzbiór danych treningowych oraz losowy podzbiór cech do rozważenia przy każdym podziale węzła.
Ten proces losowania danych i cech wprowadza różnorodność między drzewami, co przekłada się na lepszą ogólną wydajność modelu.
Poszczególne drzewa dzielone są na podstawie kryteriów omówionych w sekcji dotyczącej drzew decyzyjnych, takich jak entropia, wskaźnik Gini czy zysk informacji \citep{breiman2001}.

\vspace{0.5cm}
Dodatkowo przy budownie lasu losowego dobiera się parametry:

\vspace{0.5cm}
\begin{table}[ht]
\centering
\caption{Podstawowe hiperparametry lasu losowego}
\label{tab:rf_params}
\begin{tabular}{|l|p{10cm}|}
\hline
\textbf{Parametr} & \textbf{Opis} \\
\hline
Liczba drzew (\(B\)) & Liczba drzew decyzyjnych w lesie. Większa liczba zazwyczaj poprawia wydajność, ale zwiększa czas obliczeń. \\
\hline
Maksymalna głębokość & Maksymalna głębokość każdego drzewa. Ograniczenie głębokości może zapobiec przeuczeniu. \\
\hline
Liczba cech (\(m\)) & Liczba losowo wybranych cech rozważanych przy każdym podziale. Typowo \(m = \sqrt{p}\) dla klasyfikacji lub \(m = p/3\) dla regresji. (p to liczba wszystkich cech) \\
\hline
Minimalna liczba próbek & Minimalna liczba próbek wymagana do podziału węzła wewnętrznego lub do utworzenia liścia. \\
\hline
Bootstrap & Czy stosować bootstrap sampling (losowanie ze zwracaniem) do tworzenia podzbiorów treningowych. \\
\hline
\end{tabular}
\end{table}

Strojenie hiperparametrów lasu losowego, jest zadaniem człowieka, ale najczęściej wykorzystuje się metody automatyczne do testowania.

\vspace{0.5cm}
\textbf{Przeszukiwanie Siatki (Grid Search)}
Grid Search polega na przeszukiwaniu wszystkich możliwych kombinacji hiperparametrów z wcześniej zdefiniowanej siatki wartości.
Dla każdej kombinacji algorytm trenuje model i ocenia jego wydajność za pomocą walidacji krzyżowej.
Następnie wybiera zestaw parametrów dający najlepsze wyniki według ustalonej metryki.

\vspace{0.5cm}
\textbf{przeszukiwanie losowe (Randomized Search)}
Randomized Search jest wariantem Grid Search, który zamiast sprawdzać wszystkie kombinacje, losowo próbkuje określoną liczbę zestawów hiperparametrów z zadanych rozkładów.
Liczba iteracji jest definiowana przez programistę.

\vspace{0.5cm}
\textbf{Optuna}
Optuna to framework do optymalizacji hiperparametrów wykorzystujący zaawansowane algorytmy, takie jak Tree-structured Parzen Estimator (TPE). 
W przeciwieństwie do metod grid i random search, Optuna adaptacyjnie wybiera kolejne kombinacje hiperparametrów na podstawie wyników wcześniejszych prób.
Uczy się, które obszary przestrzeni są obiecujące i koncentruje tam przeszukiwanie.
Framework oferuje elastyczność w definiowaniu przestrzeni parametrów, wbudowane wizualizacje oraz możliwość przycinania nieobiecujących prób \citep{akiba2019}.

\vspace{0.5cm}
\textbf{Optymalizacja bayesowska (Bayesian Optimization)}
Bayesian Optimization buduje probabilistyczny model zastępczy (surrogate model), zazwyczaj Gaussian Process, który aproksymuje funkcję celu (np. dokładność modelu jako funkcję hiperparametrów).
Na podstawie tego modelu algorytm wybiera kolejne punkty do próbkowania za pomocą funkcji akwizycji (acquisition function),
takiej jak Expected Improvement (EI), która balansuje eksplorację nowych obszarów przestrzeni i eksploatację obiecujących regionów \citep{snoek2012}.

\vspace{0.5cm}
\textbf{AutoML (Automated Machine Learning)}
AutoML automatyzuje cały proces budowy modelu uczenia maszynowego, w tym wybór algorytmu, inżynierię cech, dobór hiperparametrów oraz tworzenie modeli zespołowych.
Narzędzia takie jak Auto-sklearn, TPOT czy H2O AutoML wykorzystują kombinację technik optymalizacji (bayesian optimization, evolutionary algorithms) oraz wiedzę z wcześniejszych eksperymentów na podobnych zbiorach
(meta-learning) \citep{feurer2015,olson2016}.

\subsection{Maszyna wektorów nośnych (Support Vector Machine, SVM)}
Maszyna wektorów nośnych (SVM) to algorytm uczenia maszynowego stosowany do klasyfikacji i regresji, który działa na zasadzie znajdowania optymalnej hiperpłaszczyzny rozdzielającej dane należące do różnych klas w przestrzeni wielowymiarowej.
Celem SVM jest maksymalizacja marginesu, czyli odległości między hiperpłaszczyzną a najbliższymi punktami z obu klas, zwanymi wektorami nośnymi (support vectors).
Większy margines prowadzi do lepszej generalizacji modelu na nowe, nieznane dane \citep{bishop2006,hastie2009}.

\vspace{0.5cm}
\textbf{Zasada działania.}
Dla liniowo separowalnych danych, SVM znajduje hiperpłaszczyznę definiowaną jako:
\begin{equation}
\boldsymbol{w}^\top \boldsymbol{x} + b = 0
\end{equation}
gdzie \(\boldsymbol{w}\) to wektor normalny do hiperpłaszczyzny, \(\boldsymbol{x}\) to wektor cech, a \(b\) to wyraz wolny. Funkcja decyzyjna klasyfikuje punkt \(\boldsymbol{x}\) na podstawie znaku wyrażenia \(\boldsymbol{w}^\top \boldsymbol{x} + b\):
\begin{equation}
f(\boldsymbol{x}) = \text{sign}(\boldsymbol{w}^\top \boldsymbol{x} + b)
\end{equation}

Optymalna hiperpłaszczyzna jest wyznaczana przez rozwiązanie problemu optymalizacji:
\begin{equation}
\min_{\boldsymbol{w}, b} \frac{1}{2} \|\boldsymbol{w}\|^2 \quad \text{przy ograniczeniach} \quad y_i(\boldsymbol{w}^\top \boldsymbol{x}_i + b) \geq 1, \; \forall i
\end{equation}
gdzie \(y_i \in \{-1, +1\}\) to etykiety klas, a \(\boldsymbol{x}_i\) to wektory treningowe. Problem ten jest rozwiązywany za pomocą metod programowania kwadratowego lub przez przekształcenie do postaci dualnej z wykorzystaniem mnożników Lagrange'a \citep{bishop2006}.

\vspace{0.5cm}
\textbf{Soft Margin SVM.}
W praktyce dane często nie są liniowo separowalne lub zawierają szumy. W takich przypadkach stosuje się wariant soft margin SVM, który dopuszcza błędy klasyfikacji poprzez wprowadzenie zmiennych slackowych \(\xi_i \geq 0\). Problem optymalizacji przyjmuje wtedy postać:
\begin{equation}
\min_{\boldsymbol{w}, b, \boldsymbol{\xi}} \frac{1}{2} \|\boldsymbol{w}\|^2 + C \sum_{i=1}^{n} \xi_i
\end{equation}
przy ograniczeniach:
\begin{equation}
y_i(\boldsymbol{w}^\top \boldsymbol{x}_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0, \; \forall i
\end{equation}
gdzie \(C > 0\) to parametr regularyzacji kontrolujący kompromis między maksymalizacją marginesu a minimalizacją błędów klasyfikacji. Niskie wartości \(C\) preferują większy margines (tolerancja na błędy), wysokie wartości \(C\) zmuszają model do dokładniejszego dopasowania danych treningowych \citep{hastie2009,james2013}.

\vspace{0.5cm}
\textbf{Kernel Trick (sztuczka jądrowa).}
Gdy dane nie są liniowo separowalne w oryginalnej przestrzeni cech, SVM wykorzystuje funkcje jądrowe (kernel functions) do mapowania danych do przestrzeni o wyższym wymiarze, w której separacja liniowa staje się możliwa. Najpopularniejsze funkcje jądrowe to:


\textbf{Jądro liniowe} — dla liniowo separowalnych danych:
\begin{equation}
K(\boldsymbol{x}_i, \boldsymbol{x}_j) = \boldsymbol{x}_i^\top \boldsymbol{x}_j
\end{equation}

\textbf{Jądro wielomianowe} — pozwala na nieliniowe granice decyzyjne:
\begin{equation}
K(\boldsymbol{x}_i, \boldsymbol{x}_j) = (\gamma \boldsymbol{x}_i^\top \boldsymbol{x}_j + r)^d
\end{equation}
gdzie \(d\) to stopień wielomianu, \(\gamma\) to parametr skalowania, a \(r\) to wyraz wolny.  

\textbf{Jądro radialnej funkcji bazowej (RBF, Gaussian)} — najczęściej stosowane, elastyczne:
\begin{equation}
K(\boldsymbol{x}_i, \boldsymbol{x}_j) = \exp\left(-\gamma \|\boldsymbol{x}_i - \boldsymbol{x}_j\|^2\right)
\end{equation}
gdzie \(\gamma > 0\) kontroluje zasięg wpływu pojedynczych punktów treningowych. Małe \(\gamma\) daje szerokie „dzwony" (prostsze modele), duże \(\gamma\) — wąskie (ryzyko przeuczenia).

\textbf{Jądro sigmoidalne} — zachowuje się podobnie do sieci neuronowych:
\begin{equation}
K(\boldsymbol{x}_i, \boldsymbol{x}_j) = \tanh(\gamma \boldsymbol{x}_i^\top \boldsymbol{x}_j + r)
\end{equation}

Dzięki sztuczce jądrowej SVM może modelować złożone, nieliniowe granice decyzyjne bez jawnego obliczania transformacji do wyższego wymiaru, co jest kosztowne obliczeniowo \citep{bishop2006,murphy2012}.

\subsection{Naiwny Klasyfikator Bayesowski (Naive Bayes)}
Naiwny klasyfikator Bayesowski to probabilistyczny algorytm klasyfikacji oparty na twierdzeniu Bayesa z założeniem warunkowej niezależności cech. 
Algorytm oblicza prawdopodobieństwo przynależności obiektu do każdej z klas na podstawie wartości jego cech.
Po obliczeniu przypisuje obiekt do klasy o najwyższym prawdopodobieństwie po uwzględnieniu danych.
Pomimo upraszczającego założenia o niezależności cech, Naive Bayes często osiąga dobre wyniki, szczególnie w zadaniach klasyfikacji tekstu i filtrowania spamu \citep{bishop2006,murphy2012}.

\vspace{0.5cm}
\textbf{Twierdzenie Bayesa.}
Podstawą algorytmu jest twierdzenie Bayesa, które wyraża prawdopodobieństwo przynależności obiektu do klasy \(C_k\) przy danych cechach \(\boldsymbol{x} = (x_1, x_2, \ldots, x_n)\):
\begin{equation}
P(C_k|\boldsymbol{x}) = \frac{P(\boldsymbol{x}|C_k) P(C_k)}{P(\boldsymbol{x})}
\end{equation}

gdzie \(P(C_k|\boldsymbol{x})\) oznacza prawdopodobieństwo a posteriori (po uwzględnieniu danych) przynależności do klasy \(C_k\) przy danych cechach \(\boldsymbol{x}\),
\(P(\boldsymbol{x}|C_k)\) to prawdopodobieństwo wystąpienia cech \(\boldsymbol{x}\) w klasie \(C_k\) (tzw. likelihood),
a \(P(C_k)\) to prawdopodobieństwo a priori (przed zobaczeniem danych) klasy \(C_k\). Mianownik \(P(\boldsymbol{x})\) jest prawdopodobieństwem wystąpienia cech \(\boldsymbol{x}\) i pełni rolę stałej normalizującej.

\vspace{0.5cm}
\textbf{Warianty algorytmu.}
W zależności od charakteru danych stosuje się różne warianty Naive Bayes:

\vspace{0.5cm}
\textbf{Gaussian Naive Bayes.}
Wariant stosowany dla cech ciągłych, zakłada, że wartości każdej cechy w danej klasie mają rozkład normalny (Gaussa), a
parametry rozkładu (średnia \(\mu_k\) i wariancja \(\sigma_k^2\)) są estymowane z danych treningowych dla każdej cechy i każdej klasy.
Gaussowy wariant naiwnego klasyfikatora Bayesowskiego jest szczególnie skuteczny, gdzie cechy są liczbami rzeczywistymi, takimi jak pomiary fizyczne czy dane sensoryczne \citep{bishop2006}.

\vspace{0.5cm}
\textbf{Multinomial Naive Bayes.}
Wariant przeznaczony dla cech dyskretnych reprezentujących liczby wystąpień zdarzeń, takich jak częstotliwość występowania słów w dokumentach tekstowych.
Modeluje prawdopodobieństwo wystąpienia danej liczby zdarzeń zgodnie z rozkładem wielomianowym.
Algorytmem jest szczególnie efektywny w zadaniach, gdzie dane są reprezentowane jako wektory liczników.
Ten wariant jest często stosowany do klasyfikacji tekstów, filtrowania spamu oraz analizie sentymentu, gdzie każda cecha reprezentuje liczbę wystąpień określonego słowa\citep{murphy2012}.

\vspace{0.5cm}
\textbf{Bernoulli Naive Bayes.}
Wariant dla cech binarnych przyjmujących wartości 0 lub 1.
W odróżnieniu od wariantu wielomianowego, który liczy wystąpienia, Bernoulli Naive Bayes modeluje jedynie fakt obecności cechy.
Jest stosowany w klasyfikacji dokumentów, gdzie cechy wskazują, czy dane słowo występuje w dokumencie, niezależnie od liczby jego wystąpień \citep{murphy2012}.

\section{Uczenie nienadzorowane (Unsupervised Learning)}
Uczenie nienadzorowane to rodzaj uczenia maszynowego, który sam odkrywa wzorce i struktury danych bez ówcześnie nadachynych etykiek przez człowieka.
Alogrytm przetwarza surowe dane wejściowe, a następnie za pomocą metod statystycznych i geometrycznych grupuje je na podstawie podobieństw lub różnic.
Po grupowaniu algorytm redukuje liczbę wymiarów danych, zachowując najważniejsze cechy i wzorce.
Uczenie nienadzorowane dzieli się na dwie główne kategorie: klasteryzację i redukcję wymiarowości \citep{bishop2006,hastie2009}.

\vspace{0.5cm}
\textbf{Klasteryzacja (Clustering)}
Klasteryzacja to technika uczenia nienadzorowanego, która polega na grupowaniu podobnych obiektów w zbiory zwane klastrami.
Celem klasteryzacji jest identyfikacja naturalnych struktur w danych, gdzie obiekty w tym samym klastrze są bardziej podobne do siebie niż do obiektów z innych klastrów.
Algorytmy klasteryzacji wykorzystują różne metryki odległości do oceny podobieństwa między obiektami, takie jak metryka euklidesowa, Manhattan czy kosinusowa, które zostały omówione w sekcji dotyczącej algorytmu \(k\)-najbliższych sąsiadów \citep{duda2001,hastie2009}.

\vspace{0.5cm}
\textbf{Redukcja wymiarowości (Dimensionality Reduction)}
Redukcja wymiarowości to technika uczenia nienadzorowanego, która polega na zmniejszeniu liczby cech w zbiorze danych przy jednoczesnym zachowaniu jak największej ilości istotnej informacji.
Celem redukcji wymiarowości jest uproszczenie modelu, poprawa wydajności obliczeniowej oraz eliminacja szumów i nadmiarowości w danych.
Popularne metody redukcji wymiarowości to analiza głównych składowych (PCA) oraz t-SNE (t-distributed Stochastic Neighbor Embedding) \citep{bishop2006,murphy2012}.

\subsection{k-Średnich (k-Means)}
Algorytm \(k\)-średnich to jedna z metod klasteryzacji, która grupuje dane wejściowe w \(k\) klastrów na podstawie podobieństwa między obiektami.
Algorytm działa iteracyjnie, przypisując każdy obiekt do najbliższego centroidu (środka klastra) i aktualizując położenie centroidów na podstawie średnich wartości obiektów w każdym klastrze.
Proces ten powtarza się, aż do osiągnięcia zbieżności, czyli gdy przypisania obiektów do klastrów przestają się zmieniać \citep{bishop2006,hastie2009}.

\begin{equation}
J = \sum_{j=1}^{k} \sum_{\boldsymbol{x}_i \in C_j} \|\boldsymbol{x}_i - \boldsymbol{\mu}_j\|^2
\end{equation}
gdzie \(J\) to całkowita suma kwadratów błędów, które są minimalizowane iteracyjnie przez algorytm, \(j\) to iteracja, \(k\) to liczba klastrów, \(\boldsymbol{x}_i\) to wektor cech obiektu \(i\), \(C_j\) to zbiór obiektów przypisanych do klastra \(j\), a \(\boldsymbol{\mu}_j\) to centroid klastra \(j\).

W algorytmie \(k\)-średnich dobiera sie następujące parametry:

\vspace{0.5cm}
\begin{table}[H]
\centering
\caption{Podstawowe parametry algorytmu \(k\)-średnich}
\label{tab:km_params}
\begin{tabular}{|l|p{10cm}|}
\hline
\textbf{Parametr} & \textbf{Opis} \\
\hline
Liczba klastrów (\(k\)) & Liczba klastrów, na które mają być podzielone dane. Wybór odpowiedniej wartości \(k\) jest kluczowy dla jakości klasteryzacji (np. metoda łokcia). \\
\hline
Inicjalizacja centroidów & Metoda wyboru początkowych pozycji centroidów (np.metoda \(k\)-means++). \\
\hline
Maksymalna liczba iteracji & Maksymalna liczba iteracji, które algorytm wykona przed zatrzymaniem. \\
\hline
Metryka odległości & Metryka używana do obliczania odległości między obiektami a centroidami, (np. metryka euklidesowa, Manhattan czy kosinusowa, które zostały opisane w sekcji dotyczącej algorytmu \(k\)-najbliższych sąsiadów). \\
\hline
\end{tabular}
\end{table}

\vspace{0.5cm}
\textbf{Metoda łokcia (Elbow Method).}
Metoda łokcia polega na uruchomieniu algorytmu \(k\)-średnich dla różnych wartości \(k\) i obliczeniu sumy kwadratów błędów (SSE) dla każdej wartości.
Następnie wykreśla się wykres SSE w funkcji \(k\) i szuka punktu, w którym dalsze zwiększanie \(k\) prowadzi do niewielkiej redukcji SSE, tworząc charakterystyczny "łokieć" na wykresie.

\vspace{0.5cm}
\textbf{Metoda \(k\)-means++.}
Metoda wyboru centroidów \(k\)-means++ polega na wyborze początkowych centroidów, aby przyspieszyć zbieżność algorytmu i poprawić jakość klasteryzacji.
Pierwszy centroid jest wybierany losowo z danych, a kolejne centroidy są wybierane z prawdopodobieństwem proporcjonalnym do kwadratu odległości od najbliższego już wybranego centroidu.
Ta metoda zmniejsza ryzyko złego rozmieszczenia początkowych centroidów, co może prowadzić do gorszych wyników klasteryzacji \citep{arthur2007}.

\vspace{0.5cm}
\subsection{Hierarchiczna klasteryzacja (Hierarchical Clustering)}
Hierarchiczna klasteryzacja to metoda klasteryzacji, która tworzy hierarchię klastrów poprzez iteracyjne łączenie lub dzielenie grup obiektów na podstawie ich podobieństwa.
Istnieją dwa główne podejścia do hierarchicznej klasteryzacji: aglomeracyjne (bottom-up) i dzielące (top-down).
W podejściu aglomeracyjnym każdy obiekt zaczyna jako oddzielny klaster, a następnie iteracyjnie łączy się najbliższe klastry, aż do osiągnięcia jednej grupy lub określonej liczby klastrów.
W podejściu dzielącym cały zbiór danych zaczyna jako jeden klaster, który jest następnie dzielony na mniejsze klastry na podstawie podobieństwa obiektów \citep{hastie2009,bishop2006}.

\vspace{0.5cm}
Hierarchiczna klasteryzacja wykorzystuje różne metryki odległości do oceny podobieństwa między obiektami lub klastrami, takie jak metryka euklidesowa, Manhattan czy kosinusowa, które zostały omówione w sekcji dotyczącej algorytmu \(k\)-najbliższych sąsiadów \citep{duda2001,hastie2009}.
W hierarchicznej klasteryzacji stosuje się również różne metody łączenia klastrów, takie jak:

\vspace{0.5cm}
\textbf{Metoda pojedynczego łączenia (Single Linkage).}
Metoda pojedynczego łączenia definiuje odległość między dwoma klastrami jako minimalną odległość między dowolnymi dwoma obiektami z tych klastrów:
\begin{equation}
d(C_i, C_j) = \min_{\boldsymbol{x} \in C_i, \boldsymbol{y} \in C_j} d(\boldsymbol{x}, \boldsymbol{y})
\end{equation}
gdzie \(C_i\) i \(C_j\) to dwa klastry, a \(d(\boldsymbol{x}, \boldsymbol{y})\) to odległość między obiektami \(\boldsymbol{x}\) i \(\boldsymbol{y}\).
Jest to podejście zachłanne, które może prowadzić do tworzenia długich, cienkich klastrów (tzw. efekt łańcucha).

\vspace{0.5cm}
\textbf{Metoda pełnego łączenia (Complete Linkage).}
Metoda pełnego łączenia definiuje odległość między dwoma klastrami jako maksymalną odległość między dowolnymi dwoma obiektami z tych klastrów:
\begin{equation}
d(C_i, C_j) = \max_{\boldsymbol{x} \in C_i, \boldsymbol{y} \in C_j} d(\boldsymbol{x}, \boldsymbol{y})
\end{equation}
To podejście prowadzi do tworzenia bardziej zwartych klastrów, ale może być wrażliwe na odległe punkty (outliers).

\vspace{0.5cm}
\textbf{Metoda średniego łączenia (Average Linkage).}
Metoda średniego łączenia definiuje odległość między dwoma klastrami jako średnią odległość między wszystkimi parami obiektów z tych klastrów:
\begin{equation}
d(C_i, C_j) = \frac{1}{|C_i| \cdot |C_j|} \sum_{\boldsymbol{x} \in C_i} \sum_{\boldsymbol{y} \in C_j} d(\boldsymbol{x}, \boldsymbol{y})
\end{equation}
gdzie \(|C_i|\) i \(|C_j|\) to liczby obiektów w klastrach \(C_i\) i \(C_j\).
To podejście stanowi kompromis między metodą pojedynczego i pełnego łączenia, tworząc bardziej zrównoważone klastry.

\vspace{0.5cm}
\subsection{DBSCAN (Density-Based Spatial Clustering)}
DBSCAN to algorytm klasteryzacji oparty na gęstości, który grupuje razem punkty znajdujące się blisko siebie w przestrzeni cech, definiując klastry jako obszary o wysokiej gęstości punktów oddzielone od siebie obszarami o niskiej gęstości.
Algorytm DBSCAN identyfikuje klastry na podstawie dwóch głównych parametrów: promienia sąsiedztwa \(\varepsilon\) (epsilon) oraz minimalnej liczby punktów \(minPts\) wymaganej do utworzenia gęstego regionu.
Punkty są klasyfikowane jako rdzeniowe, brzegowe lub szumowe w zależności od liczby sąsiadów w promieniu \(\varepsilon\) \citep{ester1996}.

\vspace{0.5cm}
\textbf{Sąsiedztwo \(\varepsilon\).}
Dla punktu \(\boldsymbol{p}\) sąsiedztwo \(\varepsilon\) definiowane jest jako zbiór punktów:
\begin{equation}
N_\varepsilon(\boldsymbol{p}) = \{\boldsymbol{q} \in D \mid d(\boldsymbol{p}, \boldsymbol{q}) \leq \varepsilon\}
\end{equation}
gdzie \(D\) to zbiór wszystkich punktów, a \(d(\boldsymbol{p}, \boldsymbol{q})\) to odległość między punktami \(\boldsymbol{p}\) i \(\boldsymbol{q}\).

\vspace{0.5cm}
\textbf{Punkt rdzeniowy (core point).}
Punkt \(\boldsymbol{p}\) jest punktem rdzeniowym, jeśli liczba punktów w jego sąsiedztwie \(\varepsilon\) wynosi co najmniej \(minPts\):
\begin{equation}
|N_\varepsilon(\boldsymbol{p})| \geq minPts
\end{equation}

\vspace{0.5cm}
\textbf{Bezpośrednia osiągalność gęstościowa.}
Punkt \(\boldsymbol{q}\) jest bezpośrednio osiągalny gęstościowo z punktu \(\boldsymbol{p}\), jeśli \(\boldsymbol{p}\) jest punktem rdzeniowym i \(\boldsymbol{q} \in N_\varepsilon(\boldsymbol{p})\).

\vspace{0.5cm}
\textbf{Osiągalność gęstościowa.}
Punkt \(\boldsymbol{q}\) jest osiągalny gęstościowo z punktu \(\boldsymbol{p}\), jeśli istnieje łańcuch punktów \(\boldsymbol{p}_1, \boldsymbol{p}_2, \ldots, \boldsymbol{p}_n\), gdzie \(\boldsymbol{p}_1 = \boldsymbol{p}\) i \(\boldsymbol{p}_n = \boldsymbol{q}\), taki że każdy kolejny punkt jest bezpośrednio osiągalny gęstościowo z poprzedniego.

\vspace{0.5cm}
\textbf{Klasyfikacja punktów w DBSCAN.}
Algorytm DBSCAN klasyfikuje każdy punkt w zbiorze danych do jednej z trzech kategorii:

\vspace{0.5cm}
\textit{Punkt rdzeniowy (core point)}: punkt \(\boldsymbol{p}\) jest rdzeniowy, jeśli ma co najmniej \(minPts\) sąsiadów w promieniu \(\varepsilon\), tj. \(|N_\varepsilon(\boldsymbol{p})| \geq minPts\). Punkty rdzeniowe stanowią centrum klastrów i inicjują ich tworzenie.

\vspace{0.5cm}
\textit{Punkt brzegowy (border point)}: punkt należący do klastra, ale niebędący punktem rdzeniowym. Punkt brzegowy leży w sąsiedztwie \(\varepsilon\) co najmniej jednego punktu rdzeniowego, ale sam ma mniej niż \(minPts\) sąsiadów. Punkty brzegowe znajdują się na peryferiach klastrów i są przypisywane do klastra poprzez bezpośrednią osiągalność gęstościową z punktu rdzeniowego.

\vspace{0.5cm}
\textit{Punkt szumowy (noise point)}: punkt, który nie jest ani rdzeniowy, ani brzegowy. Punkt szumowy nie leży w sąsiedztwie \(\varepsilon\) żadnego punktu rdzeniowego i nie należy do żadnego klastra. Punkty szumowe reprezentują obserwacje odstające (outliers) lub szum w danych.

Dzięki tej klasyfikacji DBSCAN automatycznie identyfikuje i odrzuca punkty odstające, co czyni go odpornym na szum w danych \citep{ester1996}.

\subsection{PCA (Principal Component Analysis)}
Analiza głównych składowych (PCA) to technika redukcji wymiarowości, która przekształca oryginalne cechy danych w nowy zestaw nieskorelowanych zmiennych zwanych głównymi składowymi.
Główne składowe są liniowymi kombinacjami oryginalnych cech i są uporządkowane według wariancji, którą wyjaśniają w danych.
Pierwsza główna składowa wyjaśnia największą część wariancji, druga główna składowa wyjaśnia drugą co do wielkości część wariancji, i tak dalej.
PCA jest szeroko stosowana do wizualizacji danych, kompresji danych oraz usuwania szumów \citep{murphy2012,hastie2009}.

\vspace{0.5cm}
\textbf{Macierz kowariancji.}
Algorytm PCA rozpoczyna się od wycentrowania danych (odjęcia średniej od każdej cechy) i obliczenia macierzy kowariancji, która opisuje zależności między cechami:
\begin{equation}
\Sigma = \frac{1}{n} X^\top X
\end{equation}
gdzie \(X\) to macierz danych o wymiarach \(n \times p\) (po wycentrowaniu), gdzie \(n\) to liczba próbek, a \(p\) to liczba cech. Macierz \(\Sigma\) ma wymiary \(p \times p\).

\vspace{0.5cm}
\textbf{Rozkład własny (Eigendecomposition).}
Kolejnym krokiem jest znalezienie wektorów własnych i wartości własnych macierzy kowariancji poprzez rozwiązanie równania:
\begin{equation}
\Sigma \boldsymbol{v}_i = \lambda_i \boldsymbol{v}_i
\end{equation}
gdzie \(\boldsymbol{v}_i\) to \(i\)-ty wektor własny określający kierunek \(i\)-tej głównej składowej, a \(\lambda_i\) to odpowiadająca mu wartość własna reprezentująca wariancję wyjaśnianą przez tę składową. Wektory własne są ortogonalne, co zapewnia nieskorelowanie głównych składowych.

\vspace{0.5cm}
\textbf{Transformacja danych.}
Po uporządkowaniu wektorów własnych według malejących wartości własnych (\(\lambda_1 \geq \lambda_2 \geq \ldots \geq \lambda_p\)), dane są transformowane do nowej przestrzeni głównych składowych:
\begin{equation}
Z = XW
\end{equation}
gdzie \(W\) to macierz o wymiarach \(p \times k\) zawierająca \(k\) pierwszych wektorów własnych jako kolumny, a \(Z\) to macierz danych w nowej przestrzeni o wymiarach \(n \times k\). Wybór liczby \(k\) składowych do zachowania zależy od wymaganej ilości wyjaśnianej wariancji.

\vspace{0.5cm}
\textbf{Wariancja wyjaśniona.}
Proporcja wariancji wyjaśnianej przez \(i\)-tą główną składową wynosi:
\begin{equation}
\text{Var}_i = \frac{\lambda_i}{\sum_{j=1}^{p} \lambda_j}
\end{equation}
Suma wariancji wyjaśnianych przez pierwsze \(k\) składowych określa, jaki procent całkowitej zmienności danych został zachowany po redukcji wymiarowości. W praktyce często wybiera się \(k\) takie, aby zachować 90-95\% wariancji \citep{bishop2006,hastie2009}.

\subsection{t-SNE (t-distributed Stochastic Neighbor Embedding)}
t-SNE to technika redukcji wymiarowości, która przekształca dane wysokowymiarowe w przestrzeń niskowymiarową (zazwyczaj 2D lub 3D) w taki sposób, aby zachować lokalne struktury danych.
Algorytm t-SNE modeluje podobieństwa między punktami w oryginalnej
przestrzeni jako prawdopodobieństwa warunkowe, a następnie optymalizuje rozmieszczenie punktów w przestrzeni niskowymiarowej, minimalizując różnicę między tymi prawdopodobieństwami za pomocą dywergencji Kullbacka-Leiblera \citep{maaten2008}.

\vspace{0.5cm}
\textbf{Podobieństwa w przestrzeni wysokowymiarowej.}
W przestrzeni wysokowymiarowej podobieństwo między punktami \(\boldsymbol{x}_i\) i \(\boldsymbol{x}_j\) jest modelowane jako prawdopodobieństwo warunkowe:
\begin{equation}
p_{j|i} = \frac{\exp(-\|\boldsymbol{x}_i
  - \boldsymbol{x}_j\|^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-\|\boldsymbol{x}_i - \boldsymbol{x}_k\|^2 / 2\sigma_i^2)}
\end{equation}
gdzie \(\sigma_i\) to szerokość jądra Gaussa dla punktu \(\boldsymbol{x}_i\). Prawdopodobieństwo symetryczne definiuje się jako:
\begin{equation}
p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}
\end{equation}
gdzie \(n\) to liczba punktów danych.

\vspace{0.5cm}
\textbf{Podobieństwa w przestrzeni niskowymiarowej.}
W przestrzeni niskowymiarowej podobieństwo między punktami \(\boldsymbol{y}_i\) i \(\boldsymbol{y}_j\) jest modelowane za pomocą rozkładu t-Studenta z jednym stopniem swobody:
\begin{equation}
q_{ij} = \frac{(1 + \|\boldsymbol{y}_i - \boldsymbol{y}_j\|^2)^{-1}}{\sum_{k \neq l} (1 + \|\boldsymbol{y}_k - \boldsymbol{y}_l\|^2)^{-1}}
\end{equation}
\vspace{0.5cm}
\textbf{Optymalizacja.}
Algorytm t-SNE minimalizuje dywergencję Kullbacka-Leiblera między rozkładami \(P\) i \(Q\):
\begin{equation}
KL(P||Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}
\end{equation}
Optymalizacja jest przeprowadzana za pomocą gradientu prostego lub metod opartych na momencie, co pozwala na znalezienie układu punktów \(\boldsymbol{y}_i\) w przestrzeni niskowymiarowej, który najlepiej zachowuje lokalne struktury danych z przestrzeni wysokowymiarowej \citep{maaten2008}.

\vspace{0.5cm}
\textbf{Parametry t-SNE.}
Algorytm t-SNE posiada kilka kluczowych parametrów, które wpływają na jakość i charakter wynikowej wizualizacji:
\begin{itemize}
  \item \textbf{Perplexity:} Określa liczbę najbliższych sąsiadów branych pod uwagę przy obliczaniu podobieństw w przestrzeni wysokowymiarowej. Typowe wartości to od 5 do 50. Wyższe wartości prowadzą do bardziej globalnych struktur, podczas gdy niższe wartości skupiają się na lokalnych strukturach.
  \item \textbf{Liczba iteracji:} Określa, ile razy algorytm będzie aktualizował położenia punktów w przestrzeni niskowymiarowej. Większa liczba iteracji może prowadzić do lepszej konwergencji, ale zwiększa czas obliczeń.
  \item \textbf{Współczynnik uczenia (learning rate):} Kontroluje szybkość aktualizacji położeń punktów podczas optymalizacji. Zbyt wysoki współczynnik może prowadzić do niestabilności, podczas gdy zbyt niski może spowolnić zbieżność.
\end{itemize}
Dobranie powyższych parametrów jest głównym czynnikiem wpływajacym na czytelność i jakość wizualizacji \citep{maaten2008}.

\vspace{0.5cm}
\subsection{UMAP (Uniform Manifold Approximation and Projection)}
UMAP to technika redukcji wymiarowości i wizualizacji danych, która opiera się na teorii topologii i geometrii różniczkowej.
UMAP tworzy niskowymiarową reprezentację danych wysokowymiarowych, zachowując zarówno lokalne, jak i globalne struktury danych.
Algorytm UMAP składa się z dwóch głównych etapów: konstrukcji grafu sąsiedztwa w przestrzeni wysokowymiarowej oraz optymalizacji rozmieszczenia punktów w przestrzeni niskowymiarowej \citep{mcinnes2018}.

\vspace{0.5cm}
\textbf{Konstrukcja grafu sąsiedztwa.}
W pierwszym etapie UMAP buduje graf sąsiedztwa, w którym każdy punkt danych jest połączony z jego \(k\) najbliższymi sąsiadami.
Podobieństwo między punktami \(\boldsymbol{x}_i\) i \(\boldsymbol{x}_j\) jest modelowane za pomocą funkcji ważonej:
\begin{equation}
w_{ij} = \exp\left(-\frac{d(\boldsymbol{x}_i, \boldsymbol{x}_j) - \rho_i}{\sigma_i}\right)
\end{equation}
gdzie \(d(\boldsymbol{x}_i, \boldsymbol{x}_j)\) to odległość między punktami, \(\rho_i\) to odległość do najbliższego sąsiada punktu \(\boldsymbol{x}_i\), a \(\sigma_i\) to skalowanie lokalne kontrolujące gęstość sąsiedztwa.

\vspace{0.5cm}
\textbf{Optymalizacja w przestrzeni niskowymiarowej.}
W drugim etapie UMAP optymalizuje rozmieszczenie punktów \(\boldsymbol{y}_i\) w przestrzeni niskowymiarowej, minimalizując funkcję kosztu opartą na różnicy między grafem sąsiedztwa w przestrzeni wysokowymiarowej a grafem w przestrzeni niskowymiarowej:
\begin{equation}C = \sum_{i \neq j} \left( w_{ij} \log \frac{w_{ij}}{q_{ij}} + (1 - w_{ij}) \log \frac{1 - w_{ij}}{1 - q_{ij}} \right)
\end{equation}
gdzie \(q_{ij}\) to podobieństwo między punktami \(\boldsymbol{y}_i\) i \(\boldsymbol{y}_j\) w przestrzeni niskowymiarowej, modelowane za pomocą funkcji:
\begin{equation}
q_{ij} = \frac{1}{1 + a \|\boldsymbol{y}_i - \boldsymbol{y}_j\|^{2b}}
\end{equation}
gdzie \(a\) i \(b\) to parametry kontrolujące kształt funkcji podobieństwa.
Optymalizacja jest przeprowadzana za pomocą metod gradientu prostego lub jego wariantów, co pozwala na znalezienie układu punktów \(\boldsymbol{y}_i\) w przestrzeni niskowymiarowej, który najlepiej zachowuje struktury danych z przestrzeni wysokowymiarowej \citep{mcinnes2018}.
\vspace{0.5cm}
\textbf{Parametry UMAP.}
Algorytm UMAP posiada kilka kluczowych parametrów, które wpływają na jakość i charakter wynikowej wizualizacji:
\begin{itemize}
  \item \textbf{Liczba sąsiadów (n\_neighbors):} Określa liczbę najbliższych sąsiadów branych pod uwagę przy budowie grafu sąsiedztwa. Typowe wartości to od 5 do 50. Wyższe wartości prowadzą do bardziej globalnych struktur, podczas gdy niższe wartości skupiają się na lokalnych strukturach.
  \item \textbf{Minimalna odległość (min\_dist):} Kontroluje, jak blisko punkty mogą być rozmieszczone w przestrzeni niskowymiarowej. Niższe wartości prowadzą do bardziej skondensowanych klastrów, podczas gdy wyższe wartości rozpraszają punkty bardziej równomiernie.
  \item \textbf{Liczba wymiarów docelowych (n\_components):} Określa liczbę wymiarów w przestrzeni niskowymiarowej (zazwyczaj 2 lub 3).
  \item \textbf{Liczba iteracji (n\_epochs):} Określa, ile razy algorytm będzie aktualizował położenia punktów w przestrzeni niskowymiarowej. Większa liczba iteracji może prowadzić do lepszej konwergencji, ale zwiększa czas obliczeń.
\end{itemize}
Dopasowanie parametrów takich jak liczba sąsiadów (n\_neighbors), minimalna odległość (min\_dist), liczba wymiarów docelowych i epochs przesądza o równowadze między zachowaniem lokalnych struktur danych a globalnej topologii w wizualizacjach UMAP \citep{mcinnes2018}.